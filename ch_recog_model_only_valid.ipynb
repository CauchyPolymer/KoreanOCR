{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packs loaded\n",
      "{\n",
      "  \"key\": \"3c7a8315-87a1-409c-81a7-edcf47be80e5\",\n",
      "  \"shell_port\": 33569,\n",
      "  \"stdin_port\": 33503,\n",
      "  \"ip\": \"127.0.0.1\",\n",
      "  \"iopub_port\": 33755,\n",
      "  \"hb_port\": 34920,\n",
      "  \"control_port\": 36152,\n",
      "  \"transport\": \"tcp\",\n",
      "  \"signature_scheme\": \"hmac-sha256\",\n",
      "  \"kernel_name\": \"\"\n",
      "}\n",
      "\n",
      "Paste the above JSON into a file, and connect with:\n",
      "    $> jupyter <app> --existing <file>\n",
      "or, if you are local, you can connect with just:\n",
      "    $> jupyter <app> --existing kernel-a3f6445b-a3c1-45dd-81d2-d5544a406085.json\n",
      "or even just:\n",
      "    $> jupyter <app> --existing\n",
      "if this is the most recent Jupyter kernel you have started.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import io\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import imread\n",
    "import json\n",
    "import gzip\n",
    "import tarfile\n",
    "import random\n",
    "from hangul_utils import check_syllable, split_syllable_char, split_syllables, join_jamos\n",
    "%matplotlib inline\n",
    "plt.rcParams['image.cmap'] = 'Greys'\n",
    "\n",
    "print(\"packs loaded\")\n",
    "%connect_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar opened\n",
      "tar opened\n",
      " 0% complete (1 / 300000)\n",
      " 3% complete (10001 / 300000)\n",
      " 7% complete (20001 / 300000)\n",
      "10% complete (30001 / 300000)\n",
      "13% complete (40001 / 300000)\n",
      "17% complete (50001 / 300000)\n",
      "20% complete (60001 / 300000)\n",
      "23% complete (70001 / 300000)\n",
      "27% complete (80001 / 300000)\n",
      "30% complete (90001 / 300000)\n",
      "33% complete (100001 / 300000)\n",
      "37% complete (110001 / 300000)\n",
      "40% complete (120001 / 300000)\n",
      "43% complete (130001 / 300000)\n",
      "47% complete (140001 / 300000)\n",
      "50% complete (150001 / 300000)\n",
      "53% complete (160001 / 300000)\n",
      "57% complete (170001 / 300000)\n",
      "60% complete (180001 / 300000)\n",
      "63% complete (190001 / 300000)\n",
      "67% complete (200001 / 300000)\n",
      "70% complete (210001 / 300000)\n",
      "73% complete (220001 / 300000)\n",
      "77% complete (230001 / 300000)\n",
      "80% complete (240001 / 300000)\n",
      "83% complete (250001 / 300000)\n",
      "87% complete (260001 / 300000)\n",
      "90% complete (270001 / 300000)\n",
      "93% complete (280001 / 300000)\n",
      "97% complete (290001 / 300000)\n",
      "image loaded\n",
      "label loaded\n"
     ]
    }
   ],
   "source": [
    "en_chset = []\n",
    "en_chset.extend([\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"])\n",
    "en_chset.extend([\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\",\\\n",
    "              \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\"])\n",
    "en_chset.extend([\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\",\\\n",
    "              \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\"])\n",
    "en_chset.extend([\"(\", \")\", \"'\", \"\\\"\", \".\", \",\", \":\", \";\", \"!\", \"?\", \"/\", \"@\", \"#\", \"$\",\\\n",
    "              \"%\", \"^\", \"&\", \"*\", \"[\", \"]\", \"{\", \"}\", \"<\", \">\", \"~\", \"-\"])\n",
    "\n",
    "ko_chset_cho = [\"ㄱ\", \"ㄲ\", \"ㄴ\", \"ㄷ\", \"ㄸ\", \"ㄹ\", \"ㅁ\", \"ㅂ\", \"ㅃ\", \"ㅅ\", \"ㅆ\", \"ㅇ\", \"ㅈ\", \"ㅉ\", \"ㅊ\", \"ㅋ\", \"ㅌ\", \"ㅍ\", \"ㅎ\"]\n",
    "ko_chset_jung = [\"ㅏ\", \"ㅐ\", \"ㅑ\", \"ㅒ\", \"ㅓ\", \"ㅔ\", \"ㅕ\", \"ㅖ\", \"ㅗ\", \"ㅘ\", \"ㅙ\", \"ㅚ\", \"ㅛ\", \"ㅜ\", \"ㅝ\", \"ㅞ\", \"ㅟ\", \"ㅠ\", \"ㅡ\", \"ㅢ\", \"ㅣ\"]\n",
    "ko_chset_jong = [\"X\", \"ㄱ\", \"ㄲ\", \"ㄳ\", \"ㄴ\", \"ㄵ\", \"ㄶ\", \"ㄷ\", \"ㄹ\", \"ㄺ\", \"ㄻ\", \"ㄼ\", \"ㄽ\", \"ㄾ\", \"ㄿ\", \"ㅀ\", \"ㅁ\", \"ㅂ\", \"ㅄ\", \"ㅅ\", \"ㅆ\", \"ㅇ\", \"ㅈ\", \"ㅊ\", \"ㅋ\", \"ㅌ\", \"ㅍ\", \"ㅎ\"]\n",
    "\n",
    "# Read training and test images from file\n",
    "def get_image_index_from_file(data_path):\n",
    "    index_data = []\n",
    "    with tarfile.open(data_path, \"r:*\") as tar:\n",
    "        print(\"tar opened\")\n",
    "        ft = tar.extractfile(\"index.json\")\n",
    "        ft_str = io.TextIOWrapper(ft)\n",
    "        index_data.extend(json.load(ft_str))\n",
    "        tar.close()\n",
    "\n",
    "    # Read-stream mode r|*\n",
    "    with tarfile.open(data_path, \"r|*\") as tar:\n",
    "        print(\"tar opened\")\n",
    "        img_data = []\n",
    "        for i, member in enumerate(index_data):\n",
    "            if i%10000 == 1:\n",
    "                print(\"%2.0f%% complete (%d / %d)\" % (i / len(index_data) * 100, i, len(index_data)))\n",
    "            ti = tar.next()\n",
    "            if ti.name != member['path']:\n",
    "                print(\"ERROR: order doesn't match\")\n",
    "                break;\n",
    "            f = tar.extractfile(ti)\n",
    "            img_data.append(1 - (imread(f)/255))\n",
    "        img = np.array(img_data)\n",
    "        del img_data\n",
    "        print(\"image loaded\")\n",
    "        tar.close()\n",
    "    return (index_data, img)\n",
    "\n",
    "def get_label(index_data):\n",
    "    # len + 1 for one 'invalid' label\n",
    "    label_ko_cho = np.zeros([len(index_data), len(ko_chset_cho)+1])\n",
    "    label_ko_jung = np.zeros([len(index_data), len(ko_chset_jung)+1])\n",
    "    label_ko_jong = np.zeros([len(index_data), len(ko_chset_jong)+1])\n",
    "    label_en = np.zeros([len(index_data), len(en_chset)+1])\n",
    "    for i, member in enumerate(index_data):\n",
    "        target = member['target'] # Target Character\n",
    "        # Is Hangeul?\n",
    "        if (check_syllable(target)):\n",
    "            splited = split_syllable_char(target)\n",
    "            label_en[i][len(en_chset)] = 1\n",
    "            label_ko_cho[i][ko_chset_cho.index(splited[0])] = 1\n",
    "            label_ko_jung[i][ko_chset_jung.index(splited[1])] = 1\n",
    "            if len(splited) < 3:\n",
    "                label_ko_jong[i][0] = 1\n",
    "            else:\n",
    "                label_ko_jong[i][ko_chset_jong.index(splited[2])] = 1\n",
    "        else :\n",
    "            label_ko_cho[i][len(ko_chset_cho)] = 1\n",
    "            label_ko_jung[i][len(ko_chset_jung)] = 1\n",
    "            label_ko_jong[i][len(ko_chset_jong)] = 1\n",
    "            label_en[i][en_chset.index(target)] = 1\n",
    "            \n",
    "    # Concatenate all labels\n",
    "    label = np.concatenate((label_ko_cho, label_ko_jung, label_ko_jong, label_en), axis=1)\n",
    "    print(\"label loaded\")\n",
    "    return label\n",
    "\n",
    "def get_all():\n",
    "    index_data, img = get_image_index_from_file('data/161018_noise.tgz')\n",
    "    label = get_label(index_data)\n",
    "    return (index_data, img, label)\n",
    "        \n",
    "index_data, img, label = get_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getIndex(l, indexes):\n",
    "    return [l[i] for i in indexes]\n",
    "\n",
    "def shuffle(n, *lists):\n",
    "    perm = np.random.permutation(n)\n",
    "    lists = list(lists)\n",
    "    for i in range(len(lists)):\n",
    "        if hasattr(lists[i], \"shape\"):\n",
    "            lists[i] = lists[i][perm]\n",
    "        else:\n",
    "            lists[i] = getIndex(lists[i], perm)\n",
    "    return tuple(lists)\n",
    "\n",
    "# img, label, index_data = shuffle(img.shape[0], img, label, index_data)\n",
    "\n",
    "# print(\"shuffled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainimg = img[:-15000]\n",
    "trainlabel = label[:-15000]\n",
    "testimg = img[-15000:-3000]\n",
    "testlabel = label[-15000:-3000]\n",
    "cvimg = img[-3000:]\n",
    "cvlabel = label[-3000:]\n",
    "randidx = np.random.randint(trainimg.shape[0], size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300000, 32, 32)\n",
      "{'weight': 'NORMAL', 'target': '내', 'font': 'NanumMyeongjo', 'path': '0000000.png'}\n",
      "[ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWEAAAFfCAYAAACfj30KAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhrAkFWQIS1JBhrAkFWQIS1JB/x//5hJ4NnZy+QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f018d0aa400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(img.shape)\n",
    "plt.figure()\n",
    "plt.imshow(img[0], interpolation='none')\n",
    "print(index_data[0])\n",
    "print(label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function loaded\n"
     ]
    }
   ],
   "source": [
    "def get_batch(i, batch_size, input_var):\n",
    "    if batch_size > input_var.shape[0]:\n",
    "        return input_var\n",
    "    start = (i*batch_size)%input_var.shape[0]\n",
    "    overflow = start + batch_size - input_var.shape[0]\n",
    "    if overflow <= 0:\n",
    "        return input_var[start:start+batch_size]\n",
    "    else:\n",
    "        return np.r_[input_var[start:], input_var[:overflow]]\n",
    "    \n",
    "def flatten_cnn(layer):\n",
    "    layer_shape = layer.get_shape().as_list()\n",
    "    n_out = layer_shape[1] * layer_shape[2] * layer_shape[3]\n",
    "    return tf.reshape(layer, [-1, n_out])\n",
    "\n",
    "def build_nn(shape, X, name):\n",
    "    n_before = int(X.get_shape()[1])\n",
    "    W = tf.Variable(tf.truncated_normal([n_before, shape], stddev=0.1), name=name+\"_W\")\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[shape]), name=name+\"_b\")\n",
    "    return tf.matmul(X, W)+b\n",
    "\n",
    "def build_cnn(cnn_shape, patch_shape, X, name, stride=1):\n",
    "    n_before = int(X.get_shape()[3])\n",
    "    W = tf.Variable(tf.truncated_normal([patch_shape[0], patch_shape[1], n_before, cnn_shape], stddev=0.1),\n",
    "                   name=name+\"_W\")\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[cnn_shape]), name=name+\"_b\")\n",
    "    layer = tf.nn.relu(tf.nn.conv2d(X, W, strides=[1, stride, stride, 1], padding='SAME') + b)\n",
    "    return layer\n",
    "\n",
    "def max2d_pool(layer):\n",
    "    return tf.nn.max_pool(layer, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def slice_label(tf_label, len_tuple):\n",
    "    cur = 0\n",
    "    sliced = []\n",
    "    for l in len_tuple:\n",
    "        sliced.append(tf.slice(tf_label, [0, cur], [-1, l]))\n",
    "        cur += l\n",
    "    return tuple(sliced)\n",
    "\n",
    "print(\"function loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn_1_5_W:0\n",
      "cnn_1_5_b:0\n",
      "cnn_1_3_W:0\n",
      "cnn_1_3_b:0\n",
      "cnn_2_5_W:0\n",
      "cnn_2_5_b:0\n",
      "cnn_2_3_W:0\n",
      "cnn_2_3_b:0\n",
      "cnn_2_1_W:0\n",
      "cnn_2_1_b:0\n",
      "cnn_3_5_reduce_W:0\n",
      "cnn_3_5_reduce_b:0\n",
      "cnn_3_5_W:0\n",
      "cnn_3_5_b:0\n",
      "cnn_3_3_reduce_W:0\n",
      "cnn_3_3_reduce_b:0\n",
      "cnn_3_3_W:0\n",
      "cnn_3_3_b:0\n",
      "cnn_3_1_W:0\n",
      "cnn_3_1_b:0\n",
      "dense_1_W:0\n",
      "dense_1_b:0\n",
      "logit_W:0\n",
      "logit_b:0\n",
      "session loaded\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, [None, 32, 32])\n",
    "Y = tf.placeholder(tf.float32, [None, 160])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "Y_cho, Y_jung, Y_jong, Y_en = slice_label(Y,\n",
    "                                         (len(ko_chset_cho)+1,\n",
    "                                         len(ko_chset_jung)+1,\n",
    "                                         len(ko_chset_jong)+1,\n",
    "                                         len(en_chset)+1))\n",
    "# Small inception model\n",
    "# http://laonple.blog.me/220704822964\n",
    "X_reshape = tf.reshape(X, [-1, 32, 32, 1])\n",
    "cnn_1_5 = build_cnn(12, [5,5], X_reshape, \"cnn_1_5\")\n",
    "cnn_1_3 = build_cnn(36, [3,3], X_reshape, \"cnn_1_3\")\n",
    "cnn_1_concat = tf.concat(3, [cnn_1_5, cnn_1_3])\n",
    "cnn_1_pool = max2d_pool(cnn_1_concat) # 16 * 16 * 48\n",
    "\n",
    "cnn_2_5 = build_cnn(18, [5,5], cnn_1_pool, \"cnn_2_5\")\n",
    "cnn_2_3 = build_cnn(48, [3,3], cnn_1_pool, \"cnn_2_3\")\n",
    "cnn_2_1 = build_cnn(30, [1,1], cnn_1_pool, \"cnn_2_1\")\n",
    "cnn_2_concat = tf.concat(3, [cnn_2_5, cnn_2_3, cnn_2_1])\n",
    "cnn_2_pool = max2d_pool(cnn_2_concat) # 8 * 8 * 96\n",
    "\n",
    "cnn_3_5_reduce = build_cnn(18, [1,1], cnn_2_pool, \"cnn_3_5_reduce\")\n",
    "cnn_3_5 = build_cnn(36, [5,5], cnn_3_5_reduce, \"cnn_3_5\")\n",
    "cnn_3_3_reduce = build_cnn(64, [1,1], cnn_2_pool, \"cnn_3_3_reduce\")\n",
    "cnn_3_3 = build_cnn(96, [3,3], cnn_3_3_reduce, \"cnn_3_3\")\n",
    "cnn_3_1 = build_cnn(60, [1,1], cnn_2_pool, \"cnn_3_1\")\n",
    "cnn_3_concat = tf.concat(3, [cnn_3_5, cnn_3_3, cnn_3_1])\n",
    "cnn_3_pool = max2d_pool(cnn_3_concat) # 4 * 4 * 192\n",
    "\n",
    "dense_1 = tf.nn.relu(build_nn(1024, flatten_cnn(cnn_3_pool), \"dense_1\"))\n",
    "dropout_1 = tf.nn.dropout(dense_1, keep_prob)\n",
    "\n",
    "logit = build_nn(160, dropout_1, \"logit\")\n",
    "logit_cho, logit_jung, logit_jong, logit_en = slice_label(logit,\n",
    "                                         (len(ko_chset_cho)+1,\n",
    "                                         len(ko_chset_jung)+1,\n",
    "                                         len(ko_chset_jong)+1,\n",
    "                                         len(en_chset)+1))\n",
    "h_cho = tf.nn.softmax(logit_cho)\n",
    "h_jung = tf.nn.softmax(logit_jung)\n",
    "h_jong = tf.nn.softmax(logit_jong)\n",
    "h_en = tf.nn.softmax(logit_en)\n",
    "\n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "cost_cho = tf.nn.softmax_cross_entropy_with_logits(logit_cho, Y_cho)\n",
    "cost_jung = tf.nn.softmax_cross_entropy_with_logits(logit_jung, Y_jung)\n",
    "cost_jong = tf.nn.softmax_cross_entropy_with_logits(logit_jong, Y_jong)\n",
    "cost_en = tf.nn.softmax_cross_entropy_with_logits(logit_en, Y_en)\n",
    "cost = cost_cho + cost_jung * 1.5 + cost_jong * 0.5 + cost_en\n",
    "cost_mean = tf.reduce_mean(cost) # mean of batch set\n",
    "\n",
    "var_before_adam = tf.all_variables()\n",
    "\n",
    "train = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "correct_cho = tf.equal(tf.argmax(Y_cho,1), tf.argmax(h_cho,1))\n",
    "correct_jung = tf.equal(tf.argmax(Y_jung,1), tf.argmax(h_jung,1))\n",
    "correct_jong = tf.equal(tf.argmax(Y_jong,1), tf.argmax(h_jong,1))\n",
    "correct_two = tf.logical_or(tf.logical_and(correct_cho, tf.logical_or(correct_jung, correct_jong)),\n",
    "                           tf.logical_and(correct_jung, correct_jong))\n",
    "correct_ko = tf.logical_and(tf.logical_and(correct_cho, correct_jung), correct_jong)\n",
    "correct_en = tf.equal(tf.argmax(Y_en,1), tf.argmax(h_en,1))\n",
    "correct_all = tf.logical_and(correct_ko, correct_en)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_all, tf.float32))\n",
    "accuracy_two = tf.reduce_mean(tf.cast(correct_two, tf.float32))\n",
    "accuracy_cho = tf.reduce_mean(tf.cast(correct_cho, tf.float32))\n",
    "accuracy_jung = tf.reduce_mean(tf.cast(correct_jung, tf.float32))\n",
    "accuracy_jong = tf.reduce_mean(tf.cast(correct_jong, tf.float32))\n",
    "accuracy_ko = tf.reduce_mean(tf.cast(correct_ko, tf.float32))\n",
    "accuracy_en = tf.reduce_mean(tf.cast(correct_en, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "for v in var_before_adam:\n",
    "    print (v.name)\n",
    "print(\"session loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf function loaded\n"
     ]
    }
   ],
   "source": [
    "def init_session():\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    print(\"session initialized\")\n",
    "    \n",
    "def fetch_in_batch(fetch_tuple, imgset, labelset, i, batchsize):\n",
    "    batch_x = get_batch(i, batchsize, imgset)\n",
    "    batch_y = get_batch(i, batchsize, labelset)\n",
    "    return sess.run(fetch_tuple, feed_dict={X:batch_x, Y:batch_y, keep_prob:1})\n",
    "    \n",
    "def batch_accuracy(imgset, labelset):\n",
    "    allsize = imgset.shape[0]\n",
    "    batchsize = 100\n",
    "    batch_per_epoch = int(allsize/batchsize)\n",
    "    batch_accuracy = 0\n",
    "    for i in range(batch_per_epoch):\n",
    "        batch_accuracy += fetch_in_batch(accuracy, imgset, labelset, i, batchsize)\n",
    "    return batch_accuracy / batch_per_epoch\n",
    "\n",
    "def get_accuracy_no_batch(imgset, labelset):\n",
    "    return sess.run((accuracy, accuracy_cho, accuracy_jung, accuracy_jong, accuracy_two, accuracy_en),\n",
    "                                        feed_dict={X:imgset, Y:labelset, keep_prob:1})\n",
    "\n",
    "def get_accuracy(imgset, labelset, batch=True):\n",
    "    if batch:\n",
    "        allsize = imgset.shape[0]\n",
    "        batchsize = 100\n",
    "        batch_per_epoch = int(allsize/batchsize)\n",
    "        temp_tuple = 0, 0, 0, 0, 0, 0\n",
    "        for i in range(batch_per_epoch):\n",
    "            batch_x = get_batch(i, batchsize, imgset)\n",
    "            batch_y = get_batch(i, batchsize, labelset)\n",
    "            temp_tuple = tuple([item1 + item2 for item1, item2 in\n",
    "                                zip(temp_tuple,\n",
    "                                    get_accuracy_no_batch(batch_x, batch_y))])\n",
    "        tacc, tacc_cho, tacc_jung, tacc_jong, tacc_two, tacc_en = tuple([item / batch_per_epoch for item in temp_tuple])\n",
    "    else:\n",
    "        tacc, tacc_cho, tacc_jung, tacc_jong, tacc_two, tacc_en = get_accuracy_no_batch(imgset, labelset)\n",
    "    return tacc, tacc_cho, tacc_jung, tacc_jong, tacc_two, tacc_en\n",
    "    \n",
    "def print_accuracy(imgset, labelset, batch=True):\n",
    "    tacc, tacc_cho, tacc_jung, tacc_jong, tacc_two, tacc_en = get_accuracy(imgset, labelset, batch)\n",
    "    print (\"overall accuracy = %.3f                          \" % tacc)\n",
    "    print (\"two of three = %.3f\" % tacc_two)\n",
    "    print (\"cho = %.3f\" % tacc_cho)\n",
    "    print (\"jung = %.3f\" % tacc_jung)\n",
    "    print (\"jong = %.3f\" % tacc_jong)\n",
    "    print (\"en = %.3f\" % tacc_en)\n",
    "\n",
    "def do_training(is_console=False, lr_init = 0.003):\n",
    "    trainsize = trainimg.shape[0]\n",
    "    batchsize = 100\n",
    "    batch_per_epoch = int(trainsize/batchsize)\n",
    "    print (\"Training %d, mini-batch %d * %d\" % (trainsize, batchsize, batch_per_epoch))\n",
    "\n",
    "    lr = lr_init\n",
    "    for i in range(batch_per_epoch*5):\n",
    "        if i % 200 == 0 :\n",
    "            tacc = get_accuracy(cvimg, cvlabel, True)[0]\n",
    "            print (\"%6dth epoch : cv accuracy = %.3f                  \" % (i // batch_per_epoch, tacc))\n",
    "            \n",
    "        if i % batch_per_epoch == 0 :\n",
    "            print_accuracy(testimg, testlabel, True)\n",
    "\n",
    "        batch_x = get_batch(i, batchsize, trainimg)\n",
    "        batch_y = get_batch(i, batchsize, trainlabel)\n",
    "        cur_cost = sess.run((train, cost_mean), feed_dict={X:batch_x, Y:batch_y, keep_prob:0.5, learning_rate:lr})[1]\n",
    "        if(is_console):\n",
    "            print (\"%dth... lr = %.2e, cost = %.2e\\r\" % (i, lr, cur_cost), end=\"\")\n",
    "        lr = lr * (1 - 0.0003)\n",
    "    print(\"train complete--------------------------------\")\n",
    "    print(\"test accuracy ---\")\n",
    "    print_accuracy(testimg, testlabel, True)\n",
    "    print(\"train accuracy ---\")\n",
    "    print_accuracy(trainimg, trainlabel, True)\n",
    "    \n",
    "def error_check(chset, pred_tf, label_tf, imgset, labelset):\n",
    "    label_len = label_tf.get_shape()[1]\n",
    "    n_error = np.zeros([label_len, label_len])\n",
    "    n_all = np.zeros(label_len)\n",
    "    new_chset = chset + [\"inv\"]\n",
    "    \n",
    "    allsize = imgset.shape[0]\n",
    "    batchsize = 100\n",
    "    batch_per_epoch = int(allsize/batchsize)\n",
    "    for i in range(batch_per_epoch):\n",
    "        h, y = fetch_in_batch((pred_tf, label_tf), imgset, labelset, i, batchsize)\n",
    "        for j in range(batchsize):\n",
    "            n_all[np.argmax(y[j])] += 1\n",
    "            if (np.argmax(h[j]) != np.argmax(y[j])):\n",
    "                n_error[np.argmax(y[j])][np.argmax(h[j])] += 1\n",
    "\n",
    "    print (\"Error rate\")\n",
    "    for i, ch in enumerate(new_chset):\n",
    "        most_error = np.argmax(n_error[i])\n",
    "        print (\"%s : %2.0f%% (%4d / %4d)\" %\n",
    "               (ch, float(np.sum(n_error[i])) / n_all[i] * 100, np.sum(n_error[i]), n_all[i]), end=\"\")\n",
    "        if n_error[i][most_error] > 0:\n",
    "            print (\"%6d errors with %s\" % (n_error[i][most_error], new_chset[most_error]))\n",
    "        else:\n",
    "            print (\"\")\n",
    "            \n",
    "def save_ckpt(path):\n",
    "    saver = tf.train.Saver(var_before_adam)\n",
    "    saver.save(sess, path)\n",
    "    print(\"ckpt saved\")\n",
    "    \n",
    "def load_ckpt(path):\n",
    "    saver = tf.train.Saver(var_before_adam)\n",
    "    saver.restore(sess, path)\n",
    "    print(\"ckpt loaded\")\n",
    "            \n",
    "print(\"tf function loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall accuracy = 0.998                          \n",
      "two of three = 1.000\n",
      "cho = 1.000\n",
      "jung = 0.999\n",
      "jong = 1.000\n",
      "en = 0.999\n"
     ]
    }
   ],
   "source": [
    "print_accuracy(testimg, testlabel, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate\n",
      "0 :  0% (   0 /   31)\n",
      "1 :  3% (   1 /   34)     1 errors with l\n",
      "2 :  0% (   0 /   38)\n",
      "3 :  0% (   0 /   37)\n",
      "4 :  0% (   0 /   34)\n",
      "5 :  0% (   0 /   36)\n",
      "6 :  0% (   0 /   28)\n",
      "7 :  0% (   0 /   32)\n",
      "8 :  0% (   0 /   42)\n",
      "9 :  0% (   0 /   35)\n",
      "a :  0% (   0 /   47)\n",
      "b :  3% (   1 /   39)     1 errors with h\n",
      "c :  0% (   0 /   39)\n",
      "d :  0% (   0 /   41)\n",
      "e :  0% (   0 /   26)\n",
      "f :  0% (   0 /   30)\n",
      "g :  0% (   0 /   37)\n",
      "h :  0% (   0 /   37)\n",
      "i :  0% (   0 /   36)\n",
      "j :  3% (   1 /   31)     1 errors with i\n",
      "k :  0% (   0 /   32)\n",
      "l : 12% (   5 /   41)     4 errors with I\n",
      "m :  0% (   0 /   35)\n",
      "n :  0% (   0 /   41)\n",
      "o :  0% (   0 /   37)\n",
      "p :  0% (   0 /   32)\n",
      "q :  0% (   0 /   29)\n",
      "r :  0% (   0 /   31)\n",
      "s :  0% (   0 /   28)\n",
      "t :  0% (   0 /   29)\n",
      "u :  0% (   0 /   33)\n",
      "v :  0% (   0 /   27)\n",
      "w :  2% (   1 /   41)     1 errors with W\n",
      "x :  0% (   0 /   37)\n",
      "y :  0% (   0 /   40)\n",
      "z :  0% (   0 /   29)\n",
      "A :  0% (   0 /   19)\n",
      "B :  0% (   0 /   28)\n",
      "C :  0% (   0 /   37)\n",
      "D :  0% (   0 /   38)\n",
      "E :  0% (   0 /   29)\n",
      "F :  0% (   0 /   33)\n",
      "G :  0% (   0 /   41)\n",
      "H :  0% (   0 /   24)\n",
      "I : 10% (   4 /   42)     4 errors with l\n",
      "J :  0% (   0 /   35)\n",
      "K :  0% (   0 /   37)\n",
      "L :  0% (   0 /   32)\n",
      "M :  0% (   0 /   39)\n",
      "N :  0% (   0 /   32)\n",
      "O :  3% (   1 /   31)     1 errors with 0\n",
      "P :  0% (   0 /   35)\n",
      "Q :  0% (   0 /   26)\n",
      "R :  0% (   0 /   35)\n",
      "S :  0% (   0 /   39)\n",
      "T :  0% (   0 /   37)\n",
      "U :  0% (   0 /   27)\n",
      "V :  0% (   0 /   35)\n",
      "W :  0% (   0 /   38)\n",
      "X :  0% (   0 /   38)\n",
      "Y :  0% (   0 /   29)\n",
      "Z :  0% (   0 /   35)\n",
      "( :  0% (   0 /   32)\n",
      ") :  0% (   0 /   25)\n",
      "' :  0% (   0 /   40)\n",
      "\" :  0% (   0 /   27)\n",
      ". :  0% (   0 /   32)\n",
      ", :  0% (   0 /   40)\n",
      ": :  0% (   0 /   35)\n",
      "; :  0% (   0 /   23)\n",
      "! :  0% (   0 /   35)\n",
      "? :  0% (   0 /   31)\n",
      "/ :  0% (   0 /   34)\n",
      "@ :  0% (   0 /   36)\n",
      "# :  0% (   0 /   29)\n",
      "$ :  0% (   0 /   37)\n",
      "% :  0% (   0 /   30)\n",
      "^ :  0% (   0 /   40)\n",
      "& :  0% (   0 /   38)\n",
      "* :  0% (   0 /   32)\n",
      "[ :  0% (   0 /   36)\n",
      "] :  0% (   0 /   29)\n",
      "{ :  0% (   0 /   30)\n",
      "} :  0% (   0 /   38)\n",
      "< :  0% (   0 /   36)\n",
      "> :  0% (   0 /   33)\n",
      "~ :  0% (   0 /   45)\n",
      "- :  0% (   0 /   43)\n",
      "inv :  0% (   0 / 8991)\n"
     ]
    }
   ],
   "source": [
    "error_check(en_chset, h_en, Y_en, testimg, testlabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate\n",
      "ㅏ :  0% (   0 /  928)\n",
      "ㅐ :  0% (   1 /  675)     1 errors with ㅔ\n",
      "ㅑ :  0% (   0 /  272)\n",
      "ㅒ :  0% (   0 /   58)\n",
      "ㅓ :  0% (   1 /  776)     1 errors with ㅕ\n",
      "ㅔ :  0% (   1 /  488)     1 errors with ㅐ\n",
      "ㅕ :  0% (   0 /  468)\n",
      "ㅖ :  1% (   1 /  158)     1 errors with ㅔ\n",
      "ㅗ :  0% (   0 /  734)\n",
      "ㅘ :  0% (   1 /  286)     1 errors with ㅚ\n",
      "ㅙ :  0% (   0 /  174)\n",
      "ㅚ :  0% (   1 /  406)     1 errors with ㅘ\n",
      "ㅛ :  0% (   0 /  253)\n",
      "ㅜ :  0% (   0 /  659)\n",
      "ㅝ :  0% (   0 /  257)\n",
      "ㅞ :  0% (   0 /  147)\n",
      "ㅟ :  0% (   0 /  439)\n",
      "ㅠ :  0% (   0 /  307)\n",
      "ㅡ :  0% (   2 /  638)     2 errors with ㅜ\n",
      "ㅢ :  0% (   0 /  120)\n",
      "ㅣ :  0% (   0 /  748)\n",
      "inv :  0% (   1 / 3009)     1 errors with ㅣ\n"
     ]
    }
   ],
   "source": [
    "error_check(ko_chset_jung, h_jung, Y_jung, testimg, testlabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "tensor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
