{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import data\n",
    "import tarfile\n",
    "from scipy.ndimage import imread\n",
    "from data import en_chset, ko_chset_cho, ko_chset_jung, ko_chset_jong\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "%matplotlib inline\n",
    "plt.rcParams['image.cmap'] = 'Greys'\n",
    "\n",
    "print(\"packs loaded\")\n",
    "%connect_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BatchBuffer:\n",
    "    def __init__(self, offset, size):\n",
    "        self.offset = offset\n",
    "        self.size = size\n",
    "        self.index = 0\n",
    "    \n",
    "    def read(self, num=None):\n",
    "        raise NotImplementedError('Unimplemented')\n",
    "        \n",
    "    def seek(self, num, mode=0):\n",
    "        if mode == 0:\n",
    "            self.index = num\n",
    "        elif mode == 1:\n",
    "            self.index += num\n",
    "        elif mode == 2:\n",
    "            self.index = self.size+num\n",
    "        else:\n",
    "            raise NotImplementedError('Unimplemented')\n",
    "        \n",
    "    def tell(self):\n",
    "        return self.index\n",
    "\n",
    "class ArrayBuffer(BatchBuffer):\n",
    "    def __init__(self, array, offset, size):\n",
    "        if size < 0:\n",
    "            size = len(array) - offset\n",
    "        super().__init__(offset, size)\n",
    "        self.array = array\n",
    "    \n",
    "    def read(self, num=None):\n",
    "        if self.index >= self.size:\n",
    "            return None\n",
    "        \n",
    "        if num is None:\n",
    "            num = self.size - self.index\n",
    "            \n",
    "        ni = self.index+num\n",
    "        if ni >= self.size:\n",
    "            ni = self.size\n",
    "        ret = self.array[self.offset+self.index:self.offset+ni]\n",
    "        self.index += num\n",
    "        return ret\n",
    "    \n",
    "class TarBuffer(BatchBuffer):\n",
    "    def __init__(self, tar, offset, size):\n",
    "        self.tar = tar\n",
    "        self.members = tar.getmembers()[offset:-1]\n",
    "        if size < 0:\n",
    "            size = len(self.members)\n",
    "        else:\n",
    "            self.members = self.members[:size]\n",
    "        super().__init__(offset, size)\n",
    "        \n",
    "        self.i = 0\n",
    "    \n",
    "    def read(self, num=None):\n",
    "        if self.index >= self.size:\n",
    "            return None\n",
    "        \n",
    "        if num is None:\n",
    "            num = self.size - self.index\n",
    "            \n",
    "        ni = self.index+num\n",
    "        if ni >= self.size:\n",
    "            ni = self.size\n",
    "            \n",
    "        #start_time = time.time()\n",
    "            \n",
    "        images = [1 - (imread(self.tar.extractfile(self.members[i]))/255) for i in range(num)]\n",
    "        ret = np.array(images)\n",
    "        self.index += num\n",
    "        \n",
    "        #self.i += 1\n",
    "        #print (\"%3dth, %ss\" % (self.i, time.time()-start_time))\n",
    "        \n",
    "        return ret\n",
    "\n",
    "data_path = 'data/161020.tgz'\n",
    "tar = tarfile.open(data_path, \"r:*\")\n",
    "label = data.get_label_from_tar(tar)\n",
    "    \n",
    "trainimg = TarBuffer(tar, 15000, -1)\n",
    "trainlabel = ArrayBuffer(label, 15000, -1)\n",
    "testimg = TarBuffer(tar, 0, 12000)\n",
    "testlabel = ArrayBuffer(label, 0, 12000)\n",
    "cvimg = TarBuffer(tar, 12000, 3000)\n",
    "cvlabel = ArrayBuffer(label, 12000, 3000)\n",
    "randidx = np.random.randint(trainimg.size, size=2)\n",
    "\n",
    "print(\"data loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def flatten_cnn(layer):\n",
    "    layer_shape = layer.get_shape().as_list()\n",
    "    n_out = layer_shape[1] * layer_shape[2] * layer_shape[3]\n",
    "    return tf.reshape(layer, [-1, n_out])\n",
    "\n",
    "def build_nn(shape, X, name):\n",
    "    n_before = int(X.get_shape()[1])\n",
    "    W = tf.Variable(tf.truncated_normal([n_before, shape], stddev=0.1), name=name+\"_W\")\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[shape]), name=name+\"_b\")\n",
    "    return tf.matmul(X, W)+b\n",
    "\n",
    "def build_cnn(cnn_shape, patch_shape, X, name, stride=1):\n",
    "    n_before = int(X.get_shape()[3])\n",
    "    W = tf.Variable(tf.truncated_normal([patch_shape[0], patch_shape[1], n_before, cnn_shape], stddev=0.1),\n",
    "                   name=name+\"_W\")\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[cnn_shape]), name=name+\"_b\")\n",
    "    layer = tf.nn.relu(tf.nn.conv2d(X, W, strides=[1, stride, stride, 1], padding='SAME') + b)\n",
    "    return layer\n",
    "\n",
    "def max2d_pool(layer):\n",
    "    return tf.nn.max_pool(layer, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def slice_label(tf_label, len_tuple):\n",
    "    cur = 0\n",
    "    sliced = []\n",
    "    for l in len_tuple:\n",
    "        sliced.append(tf.slice(tf_label, [0, cur], [-1, l]))\n",
    "        cur += l\n",
    "    return tuple(sliced)\n",
    "\n",
    "print(\"function loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y_size = len(ko_chset_cho)+len(ko_chset_jung)+len(ko_chset_jong)+len(en_chset)+4 \n",
    "\n",
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, [None, 32, 32])\n",
    "Y = tf.placeholder(tf.float32, [None, Y_size])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "Y_cho, Y_jung, Y_jong, Y_en = slice_label(Y,\n",
    "                                         (len(ko_chset_cho)+1,\n",
    "                                         len(ko_chset_jung)+1,\n",
    "                                         len(ko_chset_jong)+1,\n",
    "                                         len(en_chset)+1))\n",
    "# Small inception model\n",
    "# http://laonple.blog.me/220704822964\n",
    "X_reshape = tf.reshape(X, [-1, 32, 32, 1])\n",
    "cnn_1_5 = build_cnn(12, [5,5], X_reshape, \"cnn_1_5\")\n",
    "cnn_1_3 = build_cnn(36, [3,3], X_reshape, \"cnn_1_3\")\n",
    "cnn_1_concat = tf.concat(3, [cnn_1_5, cnn_1_3])\n",
    "cnn_1_pool = max2d_pool(cnn_1_concat) # 16 * 16 * 48\n",
    "\n",
    "cnn_2_5 = build_cnn(18, [5,5], cnn_1_pool, \"cnn_2_5\")\n",
    "cnn_2_3 = build_cnn(48, [3,3], cnn_1_pool, \"cnn_2_3\")\n",
    "cnn_2_1 = build_cnn(30, [1,1], cnn_1_pool, \"cnn_2_1\")\n",
    "cnn_2_concat = tf.concat(3, [cnn_2_5, cnn_2_3, cnn_2_1])\n",
    "cnn_2_pool = max2d_pool(cnn_2_concat) # 8 * 8 * 96\n",
    "\n",
    "cnn_3_5_reduce = build_cnn(18, [1,1], cnn_2_pool, \"cnn_3_5_reduce\")\n",
    "cnn_3_5 = build_cnn(36, [5,5], cnn_3_5_reduce, \"cnn_3_5\")\n",
    "cnn_3_3_reduce = build_cnn(64, [1,1], cnn_2_pool, \"cnn_3_3_reduce\")\n",
    "cnn_3_3 = build_cnn(96, [3,3], cnn_3_3_reduce, \"cnn_3_3\")\n",
    "cnn_3_1 = build_cnn(60, [1,1], cnn_2_pool, \"cnn_3_1\")\n",
    "cnn_3_concat = tf.concat(3, [cnn_3_5, cnn_3_3, cnn_3_1])\n",
    "cnn_3_pool = max2d_pool(cnn_3_concat) # 4 * 4 * 192\n",
    "\n",
    "dense_1 = tf.nn.relu(build_nn(1024, flatten_cnn(cnn_3_pool), \"dense_1\"))\n",
    "dropout_1 = tf.nn.dropout(dense_1, keep_prob)\n",
    "\n",
    "logit = build_nn(Y_size, dropout_1, \"logit\")\n",
    "logit_cho, logit_jung, logit_jong, logit_en = slice_label(logit,\n",
    "                                         (len(ko_chset_cho)+1,\n",
    "                                         len(ko_chset_jung)+1,\n",
    "                                         len(ko_chset_jong)+1,\n",
    "                                         len(en_chset)+1))\n",
    "h_cho = tf.nn.softmax(logit_cho)\n",
    "h_jung = tf.nn.softmax(logit_jung)\n",
    "h_jong = tf.nn.softmax(logit_jong)\n",
    "h_en = tf.nn.softmax(logit_en)\n",
    "\n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "cost_cho = tf.nn.softmax_cross_entropy_with_logits(logit_cho, Y_cho)\n",
    "cost_jung = tf.nn.softmax_cross_entropy_with_logits(logit_jung, Y_jung)\n",
    "cost_jong = tf.nn.softmax_cross_entropy_with_logits(logit_jong, Y_jong)\n",
    "cost_en = tf.nn.softmax_cross_entropy_with_logits(logit_en, Y_en)\n",
    "cost = cost_cho + cost_jung * 1.5 + cost_jong * 0.5 + cost_en\n",
    "cost_mean = tf.reduce_mean(cost) # mean of batch set\n",
    "\n",
    "var_before_adam = tf.all_variables()\n",
    "\n",
    "train = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "correct_cho = tf.equal(tf.argmax(Y_cho,1), tf.argmax(h_cho,1))\n",
    "correct_jung = tf.equal(tf.argmax(Y_jung,1), tf.argmax(h_jung,1))\n",
    "correct_jong = tf.equal(tf.argmax(Y_jong,1), tf.argmax(h_jong,1))\n",
    "correct_two = tf.logical_or(tf.logical_and(correct_cho, tf.logical_or(correct_jung, correct_jong)),\n",
    "                           tf.logical_and(correct_jung, correct_jong))\n",
    "correct_ko = tf.logical_and(tf.logical_and(correct_cho, correct_jung), correct_jong)\n",
    "correct_en = tf.equal(tf.argmax(Y_en,1), tf.argmax(h_en,1))\n",
    "correct_all = tf.logical_and(correct_ko, correct_en)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_all, tf.float32))\n",
    "accuracy_two = tf.reduce_mean(tf.cast(correct_two, tf.float32))\n",
    "accuracy_cho = tf.reduce_mean(tf.cast(correct_cho, tf.float32))\n",
    "accuracy_jung = tf.reduce_mean(tf.cast(correct_jung, tf.float32))\n",
    "accuracy_jong = tf.reduce_mean(tf.cast(correct_jong, tf.float32))\n",
    "accuracy_ko = tf.reduce_mean(tf.cast(correct_ko, tf.float32))\n",
    "accuracy_en = tf.reduce_mean(tf.cast(correct_en, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "print(\"session loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def init_session():\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    print(\"session initialized\")\n",
    "    \n",
    "# Submission function for thread pool executor\n",
    "def _load_batch(imgbuf, labelbuf, batchsize):\n",
    "    return (imgbuf.read(batchsize), labelbuf.read(batchsize))\n",
    "\n",
    "# Prepare batch on another thread concurrently\n",
    "def load_batch(executor, imgbuf, labelbuf, batchsize):\n",
    "    return executor.submit(_load_batch, imgbuf, labelbuf, batchsize)    \n",
    "\n",
    "def get_accuracy_no_batch(imgset, labelset):\n",
    "    return sess.run((accuracy, accuracy_cho, accuracy_jung, accuracy_jong, accuracy_two, accuracy_en),\n",
    "                                        feed_dict={X:imgset, Y:labelset, keep_prob:1})\n",
    "\n",
    "def get_accuracy(imgbuf, labelbuf, batch=True, executor=None):\n",
    "    imgbuf.seek(0)\n",
    "    labelbuf.seek(0)\n",
    "    \n",
    "    if batch:\n",
    "        temp_tuple = 0, 0, 0, 0, 0, 0\n",
    "        i = 0\n",
    "        batch_f = load_batch(executor, imgbuf, labelbuf, 100)\n",
    "        while(True):\n",
    "            batch_x, batch_y = batch_f.result()\n",
    "            batch_f = load_batch(executor, imgbuf, labelbuf, 100)\n",
    "            if batch_x is None:\n",
    "                break\n",
    "            temp_tuple = tuple([item1 + item2 * batch_x.shape[0] for item1, item2 in\n",
    "                                zip(temp_tuple,\n",
    "                                    get_accuracy_no_batch(batch_x, batch_y))])\n",
    "            i += batch_x.shape[0]\n",
    "        tacc, tacc_cho, tacc_jung, tacc_jong, tacc_two, tacc_en = tuple([item / i for item in temp_tuple])\n",
    "    else:\n",
    "        tacc, tacc_cho, tacc_jung, tacc_jong, tacc_two, tacc_en = get_accuracy_no_batch(imgbuf.read(), labelbuf.read())\n",
    "    return tacc, tacc_cho, tacc_jung, tacc_jong, tacc_two, tacc_en\n",
    "    \n",
    "def print_accuracy(imgbuf, labelbuf, batch=True, executor=None):\n",
    "    tacc, tacc_cho, tacc_jung, tacc_jong, tacc_two, tacc_en = get_accuracy(imgbuf, labelbuf, batch, executor)\n",
    "    print (\"overall accuracy = %.3f                          \" % tacc)\n",
    "    print (\"two of three = %.3f\" % tacc_two)\n",
    "    print (\"cho = %.3f\" % tacc_cho)\n",
    "    print (\"jung = %.3f\" % tacc_jung)\n",
    "    print (\"jong = %.3f\" % tacc_jong)\n",
    "    print (\"en = %.3f\" % tacc_en)\n",
    "    \n",
    "def print_accuracy_std(imgbuf, labelbuf, batch=True):\n",
    "    if batch == True:\n",
    "        with ThreadPoolExecutor(max_workers=1) as executor:\n",
    "            print_accuracy(testimg, testlabel, True, executor)\n",
    "    else:\n",
    "        print_accuracy(testimg, testlabel, False)\n",
    "    \n",
    "def do_training(is_console=False, lr_init = 0.003):\n",
    "    trainsize = trainimg.size\n",
    "    batchsize = 100\n",
    "    batch_per_epoch = int(trainsize/batchsize)\n",
    "    print (\"Training %d, mini-batch %d * %d\" % (trainsize, batchsize, batch_per_epoch))\n",
    "    epoch = 0\n",
    "\n",
    "    i = 0\n",
    "    lr = lr_init\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=1) as executor:\n",
    "        trainimg.seek(0)\n",
    "        trainlabel.seek(0)\n",
    "        batch_f = load_batch(executor, trainimg, trainlabel, batchsize)\n",
    "        while (epoch < 4):\n",
    "            if i % 200 == 0 :\n",
    "                tacc = get_accuracy(cvimg, cvlabel, True, executor)[0]\n",
    "                print (\"%6dth epoch : cv accuracy = %.3f                  \" % (epoch, tacc))\n",
    "                \n",
    "            batch_x, batch_y = batch_f.result()\n",
    "            if batch_x is None:\n",
    "                print_accuracy(testimg, testlabel, True, executor)\n",
    "                trainimg.seek(0)\n",
    "                trainlabel.seek(0)\n",
    "                epoch += 1\n",
    "                continue\n",
    "                \n",
    "            ## Load batch must be after None check, because None check modifies buffer index\n",
    "            batch_f = load_batch(executor, trainimg, trainlabel, batchsize)\n",
    "\n",
    "            cur_cost = sess.run((train, cost_mean),\n",
    "                                feed_dict={X:batch_x, Y:batch_y, keep_prob:0.5, learning_rate:lr})[1]\n",
    "            if(is_console):\n",
    "                print (\"%dth... lr = %.2e, cost = %.2e\\r\" % (i, lr, cur_cost), end=\"\")\n",
    "            lr = lr * (1 - 0.0003)\n",
    "            i += 1\n",
    "        \n",
    "    print(\"train complete--------------------------------\")\n",
    "    print(\"test accuracy ---\")\n",
    "    print_accuracy(testimg, testlabel, True)\n",
    "    print(\"train accuracy ---\")\n",
    "    print_accuracy(trainimg, trainlabel, True)\n",
    "    \n",
    "def error_check(chset, pred_tf, label_tf, imgbuf, labelbuf):\n",
    "    label_len = label_tf.get_shape()[1]\n",
    "    n_error = np.zeros([label_len, label_len])\n",
    "    n_all = np.zeros(label_len)\n",
    "    new_chset = chset + [\"inv\"]\n",
    "    \n",
    "    batchsize = 100\n",
    "    \n",
    "    imgbuf.seek(0)\n",
    "    labelbuf.seek(0)\n",
    "    with ThreadPoolExecutor(max_workers=1) as executor:\n",
    "        batch_f = load_batch(executor, imgbuf, labelbuf, batchsize)\n",
    "        while(True):\n",
    "            batch_x, batch_y = batch_f.result()\n",
    "            batch_f = load_batch(executor, imgbuf, labelbuf, batchsize)\n",
    "            if batch_x is None:\n",
    "                break\n",
    "            h, y = sess.run((pred_tf, label_tf), feed_dict={X:batch_x, Y:batch_y, keep_prob:1})\n",
    "            for j in range(batch_x.shape[0]):\n",
    "                n_all[np.argmax(y[j])] += 1\n",
    "                if (np.argmax(h[j]) != np.argmax(y[j])):\n",
    "                    n_error[np.argmax(y[j])][np.argmax(h[j])] += 1\n",
    "\n",
    "    print (\"Error rate\")\n",
    "    for i, ch in enumerate(new_chset):\n",
    "        most_error = np.argmax(n_error[i])\n",
    "        print (\"%s : %2.0f%% (%4d / %4d)\" %\n",
    "               (ch, float(np.sum(n_error[i])) / n_all[i] * 100, np.sum(n_error[i]), n_all[i]), end=\"\")\n",
    "        if n_error[i][most_error] > 0:\n",
    "            print (\"%6d errors with %s\" % (n_error[i][most_error], new_chset[most_error]))\n",
    "        else:\n",
    "            print (\"\")\n",
    "            \n",
    "def save_ckpt(path):\n",
    "    saver = tf.train.Saver(var_before_adam)\n",
    "    saver.save(sess, path)\n",
    "    print(\"ckpt saved\")\n",
    "    \n",
    "def load_ckpt(path):\n",
    "    saver = tf.train.Saver(var_before_adam)\n",
    "    saver.restore(sess, path)\n",
    "    print(\"ckpt loaded\")\n",
    "    \n",
    "def train_and_save(path):\n",
    "    init_session()\n",
    "    do_training(True)\n",
    "    save_ckpt(path)\n",
    "            \n",
    "print(\"tf function loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_accuracy_std(testimg, testlabel, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "error_check(en_chset, h_en, Y_en, testimg, testlabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "error_check(ko_chset_jung, h_jung, Y_jung, testimg, testlabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
