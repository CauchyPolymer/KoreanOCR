{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import imread\n",
    "import json\n",
    "import gzip\n",
    "import tarfile\n",
    "import random\n",
    "from hangul_utils import check_syllable, split_syllable_char, split_syllables, join_jamos\n",
    "%matplotlib inline\n",
    "plt.rcParams['image.cmap'] = 'Greys'\n",
    "\n",
    "print(\"packs loaded\")\n",
    "%connect_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "en_chset = []\n",
    "en_chset.extend([\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"])\n",
    "en_chset.extend([\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\",\\\n",
    "              \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\"])\n",
    "en_chset.extend([\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\",\\\n",
    "              \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\"])\n",
    "en_chset.extend([\"(\", \")\", \"'\", \"\\\"\", \".\", \",\", \":\", \";\", \"!\", \"?\", \"/\", \"@\", \"#\", \"$\",\\\n",
    "              \"%\", \"^\", \"&\", \"*\", \"[\", \"]\", \"{\", \"}\", \"<\", \">\", \"~\", \"-\"])\n",
    "\n",
    "ko_chset_cho = [\"ㄱ\", \"ㄲ\", \"ㄴ\", \"ㄷ\", \"ㄸ\", \"ㄹ\", \"ㅁ\", \"ㅂ\", \"ㅃ\", \"ㅅ\", \"ㅆ\", \"ㅇ\", \"ㅈ\", \"ㅉ\", \"ㅊ\", \"ㅋ\", \"ㅌ\", \"ㅍ\", \"ㅎ\"]\n",
    "ko_chset_jung = [\"ㅏ\", \"ㅐ\", \"ㅑ\", \"ㅒ\", \"ㅓ\", \"ㅔ\", \"ㅕ\", \"ㅖ\", \"ㅗ\", \"ㅘ\", \"ㅙ\", \"ㅚ\", \"ㅛ\", \"ㅜ\", \"ㅝ\", \"ㅞ\", \"ㅟ\", \"ㅠ\", \"ㅡ\", \"ㅢ\", \"ㅣ\"]\n",
    "ko_chset_jong = [\"X\", \"ㄱ\", \"ㄲ\", \"ㄳ\", \"ㄴ\", \"ㄵ\", \"ㄶ\", \"ㄷ\", \"ㄹ\", \"ㄺ\", \"ㄻ\", \"ㄼ\", \"ㄽ\", \"ㄾ\", \"ㄿ\", \"ㅀ\", \"ㅁ\", \"ㅂ\", \"ㅄ\", \"ㅅ\", \"ㅆ\", \"ㅇ\", \"ㅈ\", \"ㅊ\", \"ㅋ\", \"ㅌ\", \"ㅍ\", \"ㅎ\"]\n",
    "\n",
    "# Read training and test images from file\n",
    "def get_image_index_from_file(indexf, dataf):\n",
    "    index_data = []\n",
    "    with gzip.open(indexf, 'rt') as arc:\n",
    "        index_data.extend(json.load(arc))\n",
    "        print(\"index loaded\")\n",
    "\n",
    "    # Read-stream mode r|*\n",
    "    with tarfile.open(dataf, \"r|*\") as tar:\n",
    "        print(\"tar opened\")\n",
    "        img_data = []\n",
    "        for i, member in enumerate(index_data):\n",
    "            if i%10000 == 1:\n",
    "                print(\"%2.0f%% complete (%d / %d)\" % (i / len(index_data) * 100, i, len(index_data)))\n",
    "            ti = tar.next()\n",
    "            if ti.name != member['path']:\n",
    "                print(\"ERROR: order doesn't match\")\n",
    "                break;\n",
    "            f = tar.extractfile(ti)\n",
    "            img_data.append(1 - (imread(f)/255))\n",
    "        img = np.array(img_data)\n",
    "        del img_data\n",
    "        print(\"image loaded\")\n",
    "    return (index_data, img)\n",
    "\n",
    "def get_label(index_data):\n",
    "    # len + 1 for one 'invalid' label\n",
    "    label_ko_cho = np.zeros([len(index_data), len(ko_chset_cho)+1])\n",
    "    label_ko_jung = np.zeros([len(index_data), len(ko_chset_jung)+1])\n",
    "    label_ko_jong = np.zeros([len(index_data), len(ko_chset_jong)+1])\n",
    "    label_en = np.zeros([len(index_data), len(en_chset)+1])\n",
    "    for i, member in enumerate(index_data):\n",
    "        target = member['target'] # Target Character\n",
    "        # Is Hangeul?\n",
    "        if (check_syllable(target)):\n",
    "            splited = split_syllable_char()\n",
    "            label_en[i][len(en_chset)] = 1\n",
    "            label_ko_cho[i][ko_chset_cho.index(splited[0])] = 1\n",
    "            label_ko_jung[i][ko_chset_jung.index(splited[1])] = 1\n",
    "            if len(splited) < 3:\n",
    "                label_ko_jong[i][0] = 1\n",
    "            else:\n",
    "                label_ko_jong[i][ko_chset_jong.index(splited[2])] = 1\n",
    "        else :\n",
    "            label_ko_cho[i][len(ko_chset_cho)] = 1\n",
    "            label_ko_jung[i][len(ko_chset_jung)] = 1\n",
    "            label_ko_jong[i][len(ko_chset_jong)] = 1\n",
    "            label_en[i][en_chset.index(target)] = 1\n",
    "            \n",
    "    # Concatenate all labels\n",
    "    label = np.concatenate((label_ko_cho, label_ko_jung, label_ko_jong, label_en), axis=1)\n",
    "    print(\"label loaded\")\n",
    "    return label\n",
    "\n",
    "def get_all():\n",
    "    index_data_en, img_en = get_image_index_from_file('data/en/index.json.gz', 'data/en/data.tar.gz')\n",
    "    index_data_ko, img_ko = get_image_index_from_file('data/ko/index.json.gz', 'data/ko/data.tar.gz')\n",
    "    index_data = index_data_en + index_data_ko\n",
    "    img = np.concatenate((img_en, img_ko))\n",
    "    label = get_label(index_data)\n",
    "    return (index_data, img, label)\n",
    "        \n",
    "index_data, img, label = get_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getIndex(l, indexes):\n",
    "    return [l[i] for i in indexes]\n",
    "\n",
    "def shuffle(n, *lists):\n",
    "    perm = np.random.permutation(n)\n",
    "    lists = list(lists)\n",
    "    for i in range(len(lists)):\n",
    "        if hasattr(lists[i], \"shape\"):\n",
    "            lists[i] = lists[i][perm]\n",
    "        else:\n",
    "            lists[i] = getIndex(lists[i], perm)\n",
    "    return tuple(lists)\n",
    "\n",
    "img, label, index_data = shuffle(img.shape[0], img, label, index_data)\n",
    "\n",
    "print(\"shuffled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(img.shape)\n",
    "plt.figure()\n",
    "plt.imshow(img[0], interpolation='none')\n",
    "print(index_data[0])\n",
    "print(label[0])\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(img[1], interpolation='none')\n",
    "print(index_data[1])\n",
    "print(label[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainimg = img_ko[:-5000]\n",
    "trainlabel = label_ko[:-5000]\n",
    "testimg = img_ko[-5000:]\n",
    "testlabel = label_ko[-5000:]\n",
    "randidx = np.random.randint(trainimg.shape[0], size=2)\n",
    "\n",
    "def get_batch(i, batch_size, input_var):\n",
    "    if batch_size > input_var.shape[0]:\n",
    "        return input_var\n",
    "    start = (i*batch_size)%input_var.shape[0]\n",
    "    overflow = start + batch_size - input_var.shape[0]\n",
    "    if overflow <= 0:\n",
    "        return input_var[start:start+batch_size]\n",
    "    else:\n",
    "        return np.r_[input_var[start:], input_var[:overflow]]\n",
    "    \n",
    "def flatten_cnn(layer):\n",
    "    layer_shape = layer.get_shape().as_list()\n",
    "    n_out = layer_shape[1] * layer_shape[2] * layer_shape[3]\n",
    "    return tf.reshape(layer, [-1, n_out])\n",
    "\n",
    "def build_nn(shape, X):\n",
    "    n_before = int(X.get_shape()[1])\n",
    "    W = tf.Variable(tf.truncated_normal([n_before, shape], stddev=0.1))\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[shape]))\n",
    "    return tf.matmul(X, W)+b\n",
    "\n",
    "def build_cnn(cnn_shape, patch_shape, X):\n",
    "    n_before = int(X.get_shape()[3])\n",
    "    W = tf.Variable(tf.truncated_normal([patch_shape[0], patch_shape[1], n_before, cnn_shape], stddev=0.1))\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[cnn_shape]))\n",
    "    layer = tf.nn.relu(tf.nn.conv2d(X, W, strides=[1, 1, 1, 1], padding='SAME') + b)\n",
    "    layer = tf.nn.max_pool(layer, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    return layer\n",
    "\n",
    "print(\"function loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, 32, 32])\n",
    "Y = tf.placeholder(tf.float32, [None, label_ko.shape[1]])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "Y_cho = tf.slice(Y, [0, 0], [-1, len(ko_chset_cho)])\n",
    "Y_jung = tf.slice(Y, [0, len(ko_chset_cho)], [-1, len(ko_chset_jung)])\n",
    "Y_jong = tf.slice(Y, [0, len(ko_chset_cho)+len(ko_chset_jung)], [-1, len(ko_chset_jong)])\n",
    "\n",
    "cnn_1 = build_cnn(32, [5,5], tf.reshape(X, [-1, 32, 32, 1]))\n",
    "cnn_2 = build_cnn(64, [5,5], cnn_1)\n",
    "\n",
    "dense_1 = tf.nn.relu(build_nn(512, flatten_cnn(cnn_2)))\n",
    "dropout_1 = tf.nn.dropout(dense_1, keep_prob)\n",
    "\n",
    "dense_2 = tf.nn.relu(build_nn(256, dropout_1))\n",
    "dropout_2 = tf.nn.dropout(dense_2, keep_prob)\n",
    "\n",
    "dense_3 = build_nn(label_ko.shape[1], dropout_2)\n",
    "h_cho = tf.nn.softmax(tf.slice(dense_3, [0, 0], [-1, len(ko_chset_cho)]))\n",
    "h_jung = tf.nn.softmax(tf.slice(dense_3, [0, len(ko_chset_cho)], [-1, len(ko_chset_jung)]))\n",
    "h_jong = tf.nn.softmax(tf.slice(dense_3, [0, len(ko_chset_cho)+len(ko_chset_jung)], [-1, len(ko_chset_jong)]))\n",
    "\n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "cost_cho = tf.reduce_mean(-tf.reduce_sum(Y_cho * tf.log(h_cho), reduction_indices=[1]))\n",
    "cost_jung = tf.reduce_mean(-tf.reduce_sum(Y_jung * tf.log(h_jung), reduction_indices=[1]))\n",
    "cost_jong = tf.reduce_mean(-tf.reduce_sum(Y_jong * tf.log(h_jong), reduction_indices=[1]))\n",
    "cost = cost_cho + cost_jung * 2 + cost_jong\n",
    "\n",
    "train = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "correct_cho = tf.equal(tf.argmax(Y_cho,1), tf.argmax(h_cho,1))\n",
    "correct_jung = tf.equal(tf.argmax(Y_jung,1), tf.argmax(h_jung,1))\n",
    "correct_jong = tf.equal(tf.argmax(Y_jong,1), tf.argmax(h_jong,1))\n",
    "correct_two = tf.logical_or(tf.logical_and(correct_cho, tf.logical_or(correct_jung, correct_jong)),\n",
    "                           tf.logical_and(correct_jung, correct_jong))\n",
    "correct_all = tf.logical_and(tf.logical_and(correct_cho, correct_jung), correct_jong)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_all, tf.float32))\n",
    "accuracy_two = tf.reduce_mean(tf.cast(correct_two, tf.float32))\n",
    "accuracy_cho = tf.reduce_mean(tf.cast(correct_cho, tf.float32))\n",
    "accuracy_jung = tf.reduce_mean(tf.cast(correct_jung, tf.float32))\n",
    "accuracy_jong = tf.reduce_mean(tf.cast(correct_jong, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "print(\"session loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def init_session():\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    print(\"session initialized\")\n",
    "    \n",
    "def train_accuracy():\n",
    "    trainsize = trainimg.shape[0]\n",
    "    batchsize = 100\n",
    "    batch_per_epoch = int(trainsize/batchsize)\n",
    "    train_accuracy = 0\n",
    "    for i in range(batch_per_epoch):\n",
    "        batch_x = get_batch(i, batchsize, trainimg)\n",
    "        batch_y = get_batch(i, batchsize, trainlabel)\n",
    "        train_accuracy += sess.run(accuracy, feed_dict={X:batch_x, Y:batch_y, keep_prob:1})\n",
    "    return train_accuracy / batch_per_epoch\n",
    "    \n",
    "def print_accuracy():\n",
    "    taccuracy, taccuracy_cho, taccuracy_jung, taccuracy_jong, taccuracy_two = \\\n",
    "            sess.run((accuracy, accuracy_cho, accuracy_jung, accuracy_jong, accuracy_two), feed_dict={X:testimg, Y:testlabel, keep_prob:1})\n",
    "    print (\"test accuracy = %.3f\" % taccuracy)\n",
    "    print (\"train accuracy = %.3f\" % train_accuracy())\n",
    "    print (\"two of three = %.3f\" % taccuracy_two)\n",
    "    print (\"cho = %.3f\" % taccuracy_cho)\n",
    "    print (\"jung = %.3f\" % taccuracy_jung)\n",
    "    print (\"jong = %.3f\" % taccuracy_jong)\n",
    "\n",
    "def do_training(is_console=False):\n",
    "    trainsize = trainimg.shape[0]\n",
    "    batchsize = 100\n",
    "    batch_per_epoch = int(trainsize/batchsize)\n",
    "    print (\"Training %d, mini-batch %d * %d\" % (trainsize, batchsize, batch_per_epoch))\n",
    "\n",
    "    lr = 0.0003\n",
    "    for i in range(batch_per_epoch*5):\n",
    "        if i % 200 == 0 :\n",
    "            taccuracy, taccuracy_cho, taccuracy_jung, taccuracy_jong, taccuracy_two = \\\n",
    "            sess.run((accuracy, accuracy_cho, accuracy_jung, accuracy_jong, accuracy_two), feed_dict={X:testimg, Y:testlabel, keep_prob:1})\n",
    "            print (\"%6dth epoch : test accuracy = %.3f\" % \\\n",
    "                   (i / batch_per_epoch, taccuracy))\n",
    "            \n",
    "        if i % batch_per_epoch == 0 :\n",
    "            print (\"                 two of three = %.3f\" % taccuracy_two)\n",
    "            print (\"                 cho = %.3f\" % taccuracy_cho)\n",
    "            print (\"                 jung = %.3f\" % taccuracy_jung)\n",
    "            print (\"                 jong = %.3f\" % taccuracy_jong)\n",
    "                   \n",
    "        \n",
    "        if(is_console):\n",
    "            print (\"%dth... lr = %f\\r\" % (i, lr), end=\"\")\n",
    "\n",
    "        batch_x = get_batch(i, batchsize, trainimg)\n",
    "        batch_y = get_batch(i, batchsize, trainlabel)\n",
    "        sess.run(train, feed_dict={X:batch_x, Y:batch_y, keep_prob:0.5, learning_rate:lr})\n",
    "        lr = lr * (1 - 0.0003)\n",
    "    print(\"train complete\")\n",
    "    print_accuracy()\n",
    "    \n",
    "def error_check(pred_label_tuple):\n",
    "    h, y = pred_label_tuple\n",
    "    n_error = np.zeros([y.shape[0], y.shape[0]])\n",
    "    n_all = np.zeros(y.shape[0])\n",
    "\n",
    "    for i in range(y.shape[0]):\n",
    "        n_all[np.argmax(y[i])] += 1\n",
    "        if (np.argmax(h[i]) != np.argmax(y[i])):\n",
    "            n_error[np.argmax(y[i])][np.argmax(h[i])] += 1\n",
    "\n",
    "\n",
    "    print (\"Error rate\")\n",
    "    for i, ch in enumerate(ko_chset_jung):\n",
    "        most_error = np.argmax(n_error[i])\n",
    "        print (\"%s : %2.0f%% (%4d / %4d)\" %\n",
    "               (ch, float(np.sum(n_error[i])) / n_all[i] * 100, np.sum(n_error[i]), n_all[i]), end=\"\")\n",
    "        if n_error[i][most_error] > 0:\n",
    "            print (\"%6d errors with %s\" % (n_error[i][most_error], ko_chset_jung[most_error]))\n",
    "        else:\n",
    "            print (\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "error_check(sess.run((h_jung, Y_jung), feed_dict={X:testimg, Y:testlabel, keep_prob:1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
